% !TeX root = ../main.tex
% -*- coding: utf-8 -*-

\chapter{协同线性流形学习算法介绍}
\label{chpt:method}
协同线性流形学习（Collaborative Linear Manifold Learning，CLML）是一个基于流形学习做链接预测的算法。
该算法首先假设了数据空间是一个低维嵌入，利用辅助矩阵和目标矩阵分别学习到了两个流形，通过“两个流形具有一致性”这个假设定义出了带低秩约束的损失函数；
最后，算法使用随机梯度下降对模型进行优化。图2.1以药物-药物关联预测为例展示了算法的主要流程，本章将分小节介绍该算法的各个部分。

\begin{figure} 
  \centering
  \includegraphics{CLML.png}
  \caption{ CLML算法主要流程}
  \label{method:fig:CLML}
\end{figure} 


\section{符号表示}
\label{method:sec:notations}

为了简化算法推导过程中对所用符号的解释并保持上下文符号一致性，本节总结了将要用到的所有重要符号并做了解释，各符号意义参见表2.1。

\begin{center}
    \label{method:table:notations}
\tablecaption{各符号释义}
\begin{tabular}{l|l} \hline
    符号 & 意义 \\ \hline
    $m$         & 目标矩阵的规模 \\
    $n$         & 辅助矩阵的规模 \\
    $D_{m*m}$   & 目标矩阵 \\
    $R_{m*n}$   & 网络间关联矩阵 \\
    $T_{n*n}$   & 辅助矩阵 \\
    $I_n$       & $n*n$的单位矩阵 \\
    $\Arrowvert{X}\Arrowvert_{F}$ & X的Frobenius范数 \\
    $\Arrowvert{X}\Arrowvert_{*}=\sum_{i}\sigma_i$ & $X$的核范数 \\
    $\sigma_i$  & 矩阵第i大的奇异值 \\
    $X\odot{Y}$ & $X$与$Y$的哈达玛积 \\
    $\langle{X},\ Y\rangle$ & X与Y的内积 \\ \hline

\end{tabular}
\end{center}

目标矩阵就是我们要对其预测的矩阵，辅助矩阵就是算法中引入的辅助信息对应的矩阵，网络间关联矩阵就是这两各矩阵间的关系。
如在DDI预测中，药物-药物关联矩阵就是目标矩阵，蛋白-蛋白关联矩阵就是辅助矩阵，药物-靶蛋白关联矩阵就是网络间关联矩阵。


\section{线性流形学习}
\subsection{先验约束}
\label{method:subsec:prior}
流形学习（manifold learning）是一类借鉴了拓扑流形概念的降维方法，它把数据看作是一个嵌入在高维空间中的低维结构。
因此，虽然数据维度可能很高或在高维空间分布很复杂，但在局部却仍具有欧式空间的性质，可利用欧式距离重新进行距离测算，
在局部建立降维映射关系来将数据映射到低维空间中。


局部线性嵌入（Locally Linear Embedding，LLE）是流形学习的一个著名算法。
LLE假设，在高维空间中的任意一个点，在局部范围内（它的k近邻点）近似位于一个超平面上，
所以该样本点可以通过其k近邻点的线性组合重构出来，即空间中的一点满足式2.1。


\begin{equation}
    \hat{X_i}=\sum_{j\in\phi(i)}\omega_{ij}X_j
\end{equation}

其中，$\phi(i)$表示点i的k近邻点集合，$\omega_{ij}$为线性重构时每个点的权重系数，为$\omega_{ij}$找到合适的值后便可令每个点都由其近邻点线性表出。


LLE算法的不足之处在于，当数据过于稀疏时容易产生“断路”，即某些点无法找到k个足够近的近邻点。
而稀疏子空间聚类（Sparsity Subspace Clustering，SSC）算法认为，
在一个子空间内的每个点都可以由其余所有点线性表出[1]。
因此为了应对数据稀疏的问题，CLML在LLE的基础上，将重构过程扩展到了全局范围内，
即每个点均使用其余所有点来线性表出而不再仅使用其k近邻点，即使其余点并未与该点直接相连，如图2.2所示。

\begin{figure}
    \centering
    \includegraphics[width=15cm, height=6cm]{global_manifold.png}
    \caption{将构建流形使用的点扩张至全局范围内}
\end{figure}

因此我们得到了式(2.2)中的对先验数据的约束，其中$W_{ij}$便是为了控制与点相连和不相连的点在重构时的权重。
$W_{ij}$的取值如式(2.3)所示，里面的$\mu$是一个介于[0, 1]内的可调节的参数，为了让直接相连的点在重构时权重更大，通常令$\mu\geq0.5$；
是一个足够大的数来保证矩阵对角线元素接近0，这保证了每个点只能由其余点来表示（而不是用自己表示）。

\begin{equation}
    f(D)=\sum^m_{i=1}\sum^m_{j=1}W_{ij}(D_{ij}-D_{0ij})
\end{equation}


\begin{equation}
    W_{ij}=\begin{cases}
        \mu,& if\ \ D_{0ij} = 1 \\
        1 - \mu,& if\ \ D_{0ij} = 0 \\
        \rho,& if\ \ i = j
        \end{cases}
\end{equation}


我们大费周章地将$D_0$矩阵中每个点都用其余点表示的目的，就是为了让之后预测出的新矩阵$D$与原矩阵$D_0$尽可能接近，从而进行链接预测的。

\subsection{重构线性流形}

在上一小节中我们得到了用$D$矩阵近似表示$D_0$所用的约束，接下来我们需要提出具体的损失函数，依靠损失函数我们才能最终学习到具体的$D$矩阵。


我们观察到，矩阵$D$、$R$的结构类似，也就是说使用$D$线性重构$R$矩阵后得到的结果仍为$m*n$的矩阵，且仍代表网络间的相互关系。
于是，我们利用这一特点对$R$和$R*D$的近似做了约束。为了防止过拟合，我们还加入了对矩阵$D$的正则化项，最终得到式(2.4)。


\begin{align}
L(D)&=\sum_{i=1}^m\Arrowvert{R_i-\sum_{j\neq{i}} R_jD_{ij} }\Arrowvert^2 + 
\alpha\sum_{i=1}^m\sum_{j=1}^mW_{ij}(D_{ij}-D_{0ij})+\gamma\Arrowvert{D}\Arrowvert_F^2\nonumber \\
    &= \Arrowvert{R-RD}\Arrowvert_F^2+\alpha\Arrowvert{W}\odot(D-D_0)\Arrowvert_F^2+\gamma\Arrowvert{D}\Arrowvert_F^2
\end{align}

式中，$\alpha$和$\beta$为超参，分别控制先验数据的约束项和正则项对损失函数的影响。最终学习到的$D$矩阵中的每个元素将是一个在[0, 1]范围内的连续数值，
做预测时可再令$P=(\arrowvert{T^*}\arrowvert+\arrowvert{T^{*^T}}\arrowvert)/2)$即可得到一个对称矩阵$P$，
其中$P_{ij}$的值代表节点$i$和节点$j$产生链接的概率，$P_{ij}$越大则两点间越可能产生链接。


\subsection{低秩约束}

如果一个矩阵的秩远小于它的维数，则可称为低秩矩阵。低秩也代表着矩阵行、列间的相关性很强，可以互相线性表出，
该特性可很好的运用在特征分析或矩阵补全（如图像修复、链接预测等）上，而在\ref{method:subsec:prior}节中提到，需要让矩阵$D$中的每个点均由其余点线性表出，
因此可以通过引入低秩约束来更好地满足这一需求。为保持矩阵低秩，可对矩阵使用$L0$范数约束，但$L0$范数的优化是个NP难问题，
故我们使用核范数来近似，也即令式X中的F范数替换为$\Arrowvert{D}\Arrowvert_*^2$。


\section{协同学习}
\label{method:sec:collaborative}
在上一节中利用$R$和$R*D$的相似性已经可以对目标矩阵进行链接预测了。但实验发现，$R$矩阵往往较为稀疏，
仅利用$R$和$R*D$之间的相似性得到的预测结果并不好，且观察后发现$R$和$R*D$之间仅有50\%左右的相似性。


但注意到，辅助矩阵$T$和$R$同样具有相似的结构，使用$T*R$组合得到的新的矩阵同先前使用$D$矩阵时一样，都应和$R$矩阵近似，因此可以很快推导出式(2.5)。

\begin{align}
L(T)&=\sum_{i=1}^m\Arrowvert{R_i-\sum_{j\neq{i}} T_{ij}R_j }\Arrowvert^2 
+\alpha\sum_{i=1}^m\sum_{j=1}^mW_{ij}(T_{ij}-T_{0ij})+\gamma\Arrowvert{T}\Arrowvert_*^2 \nonumber \\
    &=\Arrowvert{R-TR}\Arrowvert_F^2+\alpha\Arrowvert{W}\odot(T-T_0)\Arrowvert_F^2+\gamma\Arrowvert{T}\Arrowvert_*^2
\end{align}


值得注意的是，在式(2.4)和式(2.5)中使用的是同一个R矩阵。我们令$R_d=R*D$和$R_t = T*R$，则$R_d$和$R_t$都应与矩阵$R$近似，区别仅在于二者使用的是不同的先验数据。
既然$R_d$和$R_t$都与同一个$R$近似，那么理论上$R_d$和$R_t$也应具有一致性，即$R_d \approx R_t$。基于此，我们提出了如式(2.6)所示的协同学习算法的损失函数，
式中$W_1$、$W_2$的取值和作用与式(2.4), (2.5)相同，$\alpha$、$\beta$仍为超参。
\begin{align}
L(D,\ T)=&\Arrowvert{RD-RT}\Arrowvert_F^2+\alpha\Arrowvert{W_1}\odot(T-T_0)\Arrowvert_F^2 \nonumber\\
        &+\beta\Arrowvert{W_2}\odot(D-D_0)\Arrowvert_F^2+ \gamma(\Arrowvert{T}\Arrowvert_*^2+\Arrowvert{D}\Arrowvert_*^2)
\end{align}
通过协同学习的方式我们可以克服$R$矩阵数据稀疏导致重构后两矩阵差异较大（仅50\%）的问题。
通过实验发现，使用协同学习后$R_d$和$R_y$能达到接近98\%的相似度，这足以说明引入协同学习的必要性和算法的实际效果。

\section{本章小节}
本章首先提出了一个基于线性流形学习的算法，但实验后的效果并未达到预期。之后我们又考虑了辅助信息，在原来算法的基础上引入了协同学习来克服数据稀疏的影响。